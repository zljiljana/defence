Slide 1
This dissertation introduces a novel model for solving the L2 support vector machine and it is dubbed direct l2 support vector machine. This new SVM model leads to a significant training time improvment especially when it comes to larger datasets (say couple of million data points or so). In this presentation my intention is to give you some understanding of the theoretical and practical aspect of my research.
*maybe some transition here*

Slide 2
Since my research is about SVM I'd like to start of with some basic properties of SVM in general:
- used for both classification and regression analysis
- there are very powerful and also videly popular is often used in industry and academia
The main advantages:
- high generalization ability (of course, this high generalization ability can only be achieved with model selection techniques to identify the best parameters for a given dataset)
- adaptible to various problems (this means that by simply changing the kernel function we can solve quite complex problems)
- nice mathematical property (in that in SVMs we are solving this convex problem that has one global minimum which is a great advantage in respect to some other machine learning algorithms)
- BUT, the problem is that SVM does not scale well with the number of training points. in the recent years it has become apparent that datasets are growing and will continue to do so, this expedites development of algorithms that are fast and scale well with number of points. And that is what my research was motivated by and that is what Direct L2 SVM attempts to acheave)

** As a rule of thumb I use: O(n_samples^2 * n_features) for RBF kernel using SMO solver and n_sample * n_features for linear SVMs such as liblinear also noting that strong regularization (low C) makes it faster to converge too

Slide 3
OK, so, what is the solution of SVM? 
in nutshell, SVM algorithm outputs an optimal hyperplane which categorizes the points.
Let's consided the following problem, find a separating line between these points
As you can see in the picture on the left you can see that there are multiple lines that offer a solution. The questions is is any one of them better? Well, we can intuitively define a criterion to estimate the worth of the lines: the line is lets say bad if it passes too close to the points because it will be noise senstive and will not generalize correctly. So then, the goal of SVM algorithm is to find the hyperplane that gives the largest minimum distance to the training examples. We call this distance the 'margin' and we try to maximize this margin. 

Slide 4
Ok, this is a sort of a brief history of models and their representations:
- hard-margin SVM - first SVM that was developed, linear model that enabled separation of linearly separable data, in case of non-linearly separable data this model could not find the solution
So the cost function (the Q function here) that we are trying to optimize corresponds to the maximization of margin and the constraints that you see bellow ensure that we classify the training points correctly.
- Next, at the beginning of 90' a soft margin SVM was discovered which enabled separation of overlapping data and it accomplished this by simply introducing the so called slack variables which represent the errors. So the representation of this problem is given here. Basically what this means is that by including slack variables we are relaxing the constraints and allowing some errors to be made, in other words, we are allowing some points to be on the wrong side of the margin. 
However we want to limit these errors, therefore you see this additional term here which controls the trade-off between the maximization of margin and minimization of errors. The parameter C is a regularization term, which provides a way to control overfitting. 
Even with this enhancement, soft-margin SVM is still not capable of dealing with hightly complex topological structures of data. This problem was solved by mapping the input space into a high dimensional kernel space in order to enhance the linear separability of the data in that space, this process is called the kernel trick. This is the representation of kernel SVM, the only difference is that we use this function (phi) that works on our input space.
Besides these basic or original SVM models there are few variants that were developed and here I am pointing out only a few: Least Squares SVM, Proximal SVM, Geometric SVM for a few reasons, all three are very popular and widely used and more importantly they all share certain characteristics with my model - DL2 SVM. I want to mention that of all of these, geometric SVM have shown major improvements in training times which is the reason why my comprehensive model Direct L2 SVM has almost the same problem representation as geometric SVM.

Slide 5
Direct L2 SVM - I will start of this section by presenting the theory and then wrap it up with some implementation details.
I'll jump right into the cost function of my problem which you can see here. 
Most of what you see here is the same as for the soft margin L2 SVM you saw on the previous slide. The two major additions are the term involving the bias and the term involving the parameter rho. Also, on the right hand side of constraints you can see this new term rho which is just a scalar value. k_b and k_rho are scalar values and when they're fixed to one then the problem representation is the same as in geometric SVMs

Now, steps to the solution of this problem are following: 
1. we first define the primal lagrangian,
2. we define the KKT conditions (both of these steps are standard steps towards the solution for this problem posing, we see nothing different there)
3. next step is the rearrangement of these conditions 
4. and introduction of new dual variables, these two steps are new and they lead to a system of linear equations with nonegativity constraints which is a novel L2SVM model

Slide 6:

Slide 7:
Complementary conditions 
Now besides the conditions on the previous page we have complementary conditions which tell us the following,
if alpha is zero the
by inforcing non-negativity of alpha and setting constraints to equality we can find a solution to DL2 SVM cost function. We do this by plugging in expressions for omega, b, ksi, and k_rho to out complementary condition.


Algorithms: 
NNLS - active set method, iterative
CG - slightly faster version of gradient method, requires less iterations
NN ISDA

